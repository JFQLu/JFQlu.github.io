<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-W8J42SQ');</script>
    <!-- End Google Tag Manager -->
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
    <title>Tweet Sentiment Analysis</title>
    <meta name="description" content="Tweet Sentiment Analysis" />

    <link rel="stylesheet" href="css/style.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@400;600;700;900&display=swap"
      rel="stylesheet"
    />
  </head>
  <body>
    <!-- Google Tag Manager (noscript) -->
    <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W8J42SQ"
    height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
    <!-- End Google Tag Manager (noscript) -->
    <header class="header">
      <div class="header__content">
        <div class="header__logo-container">
          <div class="header__logo-img-cont">
            <img
              src="./assets/png/james_lu.png"
              alt="James Lu Logo Image"
              class="header__logo-img"
            />
          </div>
          <span class="header__logo-sub">James Lu</span>
        </div>
        <div class="header__main">
          <ul class="header__links">
            <li class="header__link-wrapper">
              <a href="./index.html" class="header__link"> Home </a>
            </li>
            <li class="header__link-wrapper">
              <a href="./index.html#about" class="header__link">About </a>
            </li>
            <li class="header__link-wrapper">
              <a href="./index.html#projects" class="header__link">
                Projects
              </a>
            </li>
            <li class="header__link-wrapper">
              <a href="./index.html#contact" class="header__link"> Contact </a>
            </li>
          </ul>
          <div class="header__main-ham-menu-cont">
            <img
              src="./assets/svg/ham-menu.svg"
              alt="hamburger menu"
              class="header__main-ham-menu"
            />
            <img
              src="./assets/svg/ham-menu-close.svg"
              alt="hamburger menu close"
              class="header__main-ham-menu-close d-none"
            />
          </div>
        </div>
      </div>
      <div class="header__sm-menu">
        <div class="header__sm-menu-content">
          <ul class="header__sm-menu-links">
            <li class="header__sm-menu-link">
              <a href="./index.html"> Home </a>
            </li>

            <li class="header__sm-menu-link">
              <a href="./index.html#about"> About </a>
            </li>

            <li class="header__sm-menu-link">
              <a href="./index.html#projects"> Projects </a>
            </li>

            <li class="header__sm-menu-link">
              <a href="./index.html#contact"> Contact </a>
            </li>
          </ul>
        </div>
      </div>
    </header>
    <!--- Hero Section
    <section class="project-cs-hero">
      <div class="project-cs-hero__content">
        <h1 class="heading-primary">Tweet Sentiment Analysis</h1>
        <div class="project-cs-hero__info">
          <p class="text-primary">
            Predictive analysis and
          </p>
        </div>
        <div class="project-cs-hero__cta">
          <a href="#" class="btn btn--bg" target="_blank">Live Link</a>
        </div>
      </div>
    </section>
    --->
    <section class="project-details">
      <div class="main-container">
        <div class="project-details__content">
          <h3 class="project-details__content-project_heading">Tweet Sentient Analysis</h3>
          <!--- Hero Image
          <div class="project-details__showcase-img-cont">
            <img
              src="./assets/jpeg/network_anomaly_detection_hero_img.jpg"
              alt="Project Image Hero"
              class="project-details__showcase-img"
            />
          </div>
          ---> 
          <div class="project-details__content-main">
            <div class="project-details__desc">
              <h3 class="project-details__content-title_1">Project Overview</h3>
              <p class="project-details__desc-para">
                In the fast-paced world of social media, it is critical to accurately identify the sentiment expressed in online communication, as it can have a significant impact on a company's brand and profitability. 
                With the constant flow of tweets, it can be difficult to determine whether a particular message will go viral in a positive way or have a negative impact on the brand. 
                Capturing and understanding the sentiment conveyed through language is crucial for making timely and informed decisions. 
                However, accurately identifying the specific words that contribute to the overall sentiment can be challenging.
              </p>
              <h3 class="project-details__content-title_1">Background</h3>
              <p class="project-details__desc-para">
                The Kaggle competition <a href="https://www.kaggle.com/competitions/tweet-sentiment-extraction/overview" target=”_blank”><u>Tweet Sentiment Extraction</u></a> embodies the fundamental ideas of sentiment analysis. 
                This challenge requires the participants to look at the labelled sentiment for a given tweet and determine the word or phrase that best supports it. 
                I will be using this data for this project. 
                For this project, I will be focussing on the classification of the sentiment for each tweet and optimisation of the implementation. 
                I believe that this deliverable would be more useful for a company looking to do sentiment analysis on their own customers.
              </p>
              <h3 class="project-details__content-title_1">Data</h3>
              <p class="project-details__desc-para">
                The raw data contains the columns textID, text, selected_text, and sentiment. This can be downloaded from Kaggle.
              </p>
              <div class="div-code">
                <p>data.info()</p>
              </div>
              <img
                src="./assets/jpeg/tsa_data_info.jpg"
                alt="Project Image Hero"
                class="project-details__showcase-img"
              />
              <p class="project-details__desc-para">
                We can remove the selected_text column since the sentiment analysis will be performed on the full text. 
                Moreover, we assign dummy variables to the sentiment:
              </p>
              <div class="div-list">
                <ul>
                  <li>
                    <p class="project-details__desc-list_ele">&bull; positive = 1</p>
                  </li>
                  <li>
                    <p class="project-details__desc-list_ele">&bull; neutral = 0</p>
                  </li>
                  <li>
                    <p class="project-details__desc-list_ele">&bull; negative = -1</p>
                  </li>
                </ul>
              </div>
              <div class="div-code">
                <p>df_train = df_train.drop(['textID', 'selected_text'], axis=1)</p>
                <p>df_train['sentiment'] = df_train['sentiment'].apply(lambda x: 1 if x == 'positive' else 0 if x == 'neutral' else -1)</p>
                <p>&nbsp;</p>
                <p>df_test = df_test.drop(['textID'], axis=1)</p>
                <p>df_test['sentiment'] = df_test['sentiment'].apply(lambda x: 1 if x == 'positive' else 0 if x == 'neutral' else -1)</p>  
              </div>
              <h3 class="project-details__content-title_1">Exploratory Data Analysis</h3>
              <p class="project-details__desc-para">
                First we look at the label distribution in the training data.
              </p>
              <div class="div-code">
                <p>label = df_train['sentiment'].value_counts()</p>
                <p>df_label = pd.DataFrame({'label_name':label.index[0:],'fre':label.values[0:]})</p>
                <p>&nbsp;</p>
                <p>plt.pie(df_label.fre,labels=df_label.label_name,autopct='%1.2f%%')</p>
                <p>plt.legend()</p>
                <p>plt.show()</p>
              </div>
              <img
                src="./assets/jpeg/tsa_eda_label_distribution.jpg"
                alt="Project Image Hero"
                class="project-details__img_small"
              />
              <p class="project-details__desc-para">
                We observe that there is 10% more neural sentiment tweets compared to the negative and positive classes which may be a potential issue biasing models toward the more prevalent class. 
                Since the difference is only 10% we will ignore this however a potential remedy for this would be to oversample the less prevelent classes or to undersample the more prevalent class.
              </p>
              <p class="project-details__desc-para">
                Now, observing tweet length distributions across sentiments,
              </p>
              <div class="div-code">
                <p>plt.figure(figsize=(16, 4))</p>
                <p>plt.subplot(1, 3, 1)</p>
                <p>df_train.query("sentiment==-1")["text"].str.len().plot(kind="hist", title="Negative")</p>
                <p>plt.xlabel('Tweets Length')plt.subplot(1, 3, 2)</p>
                <p>&nbsp;</p>
                <p>df_train.query("sentiment==0")["text"].str.len().plot(kind="hist", title="Neutral")</p>
                <p>plt.xlabel('Tweets Length')plt.subplot(1, 3, 3)</p>
                <p>&nbsp;</p>
                <p>df_train.query("sentiment==1")["text"].str.len().plot(kind="hist", title="Positive")</p>
                <p>plt.xlabel('Tweets Length')</p>
                <p>&nbsp;</p>
                <p>plt.show()</p>
              </div>
              <img
                src="./assets/jpeg/tsa_eda_tweet_length.jpg"
                alt="Project Image Hero"
                class="project-details__showcase-img"
              />
              <p class="project-details__desc-para">
                and word count distributions,
              </p>
              <div class="div-code">
                <p>plt.figure(figsize=(16, 4))</p>
                <p>plt.subplot(1, 3, 1)</p>
                <p>df_train.query("sentiment==-1").text.map(lambda x: len(x.split())).plot(kind="hist", title="Negative")</p>
                <p>plt.xlabel('Number of Words')</p>
                <p>&nbsp;</p>
                <p>plt.subplot(1, 3, 2)</p>
                <p>df_train.query("sentiment==0").text.map(lambda x: len(x.split())).plot(kind="hist", title="Neutral")</p>
                <p>plt.xlabel('Number of Words')</p>
                <p>&nbsp;</p>
                <p>plt.subplot(1, 3, 3)</p>
                <p>df_train.query("sentiment==1").text.map(lambda x: len(x.split())).plot(kind="hist", title="Positive")</p>
                <p>plt.xlabel('Number of Words')</p>
                <p>&nbsp;</p>
                <p>plt.show()</p>
              </div>
              <img
                src="./assets/jpeg/tsa_eda_tweet_length.jpg"
                alt="Project Image Hero"
                class="project-details__showcase-img"
              />
              <p class="project-details__desc-para">
                We see from this that there is little difference in terms of number of characters or words across each sentiment.
              </p>
              <h3 class="project-details__content-title_1">Preprocessing</h3>
              <h3 class="project-details__content-title_2">Decontraction</h3>
              <p class="project-details__desc-para">
                Decontraction is the process of converting contractions, which are shortened forms of words or phrases, back to their full form. For example, "I'm" would be converted to "I am".
              </p>
              <p class="project-details__desc-para">
                Decontraction is important in natural language processing (NLP) because it helps to normalize the text and make it easier for NLP models to process and understand. 
                Contractions can be ambiguous and can have multiple meanings depending on the context in which they are used. We perform decontraction manually using the following code.
              </p>
              <div class="div-code">
                <p>def decontraction_text(text):</p>
                <p>&emsp;# performing de-contraction</p>
                <p>&emsp;text = text.replace(",000,000", "m").replace(",000", "k").replace("′", "'").replace("’", "'").replace("`", "'")\</p>
                <p>&emsp;&emsp;&nbsp;&nbsp;&nbsp;.replace("won't", "will not").replace("cannot", "can not").replace("can't", "can not")\</p>
                <p>&emsp;&emsp;&nbsp;&nbsp;&nbsp;.replace("n't", " not").replace("what's", "what is").replace("it's", "it is")\</p>
                <p>&emsp;&emsp;&nbsp;&nbsp;&nbsp;.replace("'ve", " have").replace("i'm", "i am").replace("'re", " are")\</p>
                <p>&emsp;&emsp;&nbsp;&nbsp;&nbsp;.replace("he's", "he is").replace("she's", "she is").replace("'s", " is")\</p>
                <p>&emsp;&emsp;&nbsp;&nbsp;&nbsp;.replace("'m", " am").replace("'d", " would")\</p>
                <p>&emsp;&emsp;&nbsp;&nbsp;&nbsp;.replace("'ll", " will")\</p>
                <p>&emsp;&emsp;&nbsp;&nbsp;&nbsp;.replace("'ll", " will")</p>
                <p>&emsp;return text</p>
                <p>df_train['text']= df_train['text'].apply(lambda x : decontraction_text(x))</p>
                <p>df_test['text'] = df_test['text'].apply(lambda x: decontraction_text(x))</p>
                <p>df_train.head()</p>
              </div>
              <h3 class="project-details__content-title_2">Lemmatization</h3>
              <p class="project-details__desc-para">
                Lemmatization is another useful preprocessing technique for NLP. Lemmatization is the process of reducing a word to its base form, or lemma. 
                For example, the lemma of the word "was" is "be," and the lemma of the word "better" is "good."
              </p>
              <p class="project-details__desc-para">
                Lemmatization is important in natural language processing (NLP) because it helps to reduce the dimensionality of the data and make it easier for NLP models to process and understand. 
                It does this by reducing the number of unique word forms that need to be considered, which can make it easier to identify common themes and patterns in the text. 
                The code to do this is below using the nltk implementation.
              </p>
              <div class="div-code">
                <p>from nltk.stem import WordNetLemmatizer</p>
                <p>lemmatizer = WordNetLemmatizer()</p>
                <p>def lemma_stem(x):</p>
                <p>&emsp;new = ""</p>
                <p>&emsp;for y in x.split():</p>
                <p>&emsp;&emsp;lemmatizer.lemmatize(y)</p>
                <p>&emsp;&emsp;new = new + y + " "</p>
                <p>&emsp;return new.rstrip()</p>
                <p>&nbsp;</p>
                <p>df_train['text'] = df_train['text'].apply(lambda x:lemma_stem(x))</p>
                <p>df_test['text'] = df_test['text'].apply(lambda x: lemma_stem(x))</p>
              </div>
              <h3 class="project-details__content-title_2">Removing URLs</h3>
              <p class="project-details__desc-para">
                Next we can remove URLs as they do not provide useful information,
              </p>
              <div class="div-code">
                <p>def remove_URLs(text):</p>
                <p>&emsp;return re.sub(r'http\S+', ' ', text,  flags=re.MULTILINE)</p>
                <p>&emsp;for y in x.split():</p>
                <p>df_train['text']= df_train['text'].apply(lambda x : remove_URLs(x))</p>
              </div>
              <h3 class="project-details__content-title_2">Removing Punctuations</h3>
              <p class="project-details__desc-para">
                Whether or not to remove punctuations in natural language processing (NLP) tasks depends on the specific task and the type of punctuation being used. 
                In some cases, punctuations can be useful for understanding the meaning of the text and should be retained. 
                In other cases, punctuations may not be necessary and can be removed as a preprocessing step.
              </p>
              <p class="project-details__desc-para">
                In our case I choose to remove punctuations to reduce the complexity of the final models and given the context of short tweets, punctuations may not be very useful.
              </p>
              <div class="div-code">
                <p>def lower_text_and_remove_special_chars(text):</p>
                <p>&emsp;text = text.lower().strip()</p>
                <p>&emsp;return re.sub(r"\W+", " ", text)</p>
                <p>&nbsp;</p>
                <p>df_train['text']= df_train['text'].apply(lambda x : lower_text_and_remove_special_chars(x))</p>
                <p>df_test['text'] = df_test['text'].apply(lambda x: lower_text_and_remove_special_chars(x))</p>
              </div>
              <h3 class="project-details__content-title_2">Removing HTML tags</h3>
              <p class="project-details__desc-para">
                HTML tags should be removed as they do not provide information on the sentiment of a tweet.
              </p>
              <div class="div-code">
                <p>from bs4 import BeautifulSoup</p>
                <p>def remove_html_tags(text):</p>
                <p>&emsp;return BeautifulSoup(text).get_text()</p>
                <p>&nbsp;</p>
                <p>df_train['text']= df_train['text'].apply(lambda x : remove_html_tags(x))</p>
                <p>df_test['text'] = df_test['text'].apply(lambda x: remove_html_tags(x))</p>
              </div>
              <h3 class="project-details__content-title_2">Spelling correction</h3>
              <p class="project-details__desc-para">
                There may be spelling mistakes in the tweets, these should be corrected to reduce dimensionality/complexity and improve accuracy.
              </p>
              <div class="div-code">
                <p>from spellchecker import SpellChecker</p>
                <p>&nbsp;</p>
                <p>spell = SpellChecker()</p>
                <p>def correct_spellings(text):</p>
                <p>&emsp;corrected_text = []</p>
                <p>&emsp;misspelled_words = spell.unknown(text.split())</p>
                <p>&emsp;for word in text.split():</p>
                <p>&emsp;&emsp;if word in misspelled_words:</p>
                <p>&emsp;&emsp;&emsp;corrected_text.append(spell.correction(word))</p>
                <p>&emsp;&emsp;else:</p>
                <p>&emsp;&emsp;&emsp;corrected_text.append(word)</p>
                <p>&emsp;return " ".join(corrected_text)</p>
                <p>&nbsp;</p>
                <p>df_train['text']=df_train['text'].apply(lambda x : correct_spellings(x))</p>
                <p>df_test['text'] = df_test['text'].apply(lambda x: correct_spellings(x))</p>
              </div>
              <h3 class="project-details__content-title_2">Removing Stop Words</h3>
              <p class="project-details__desc-para">
                Finally, we remove stop words such as "in", "on", "with", "by" and "for" since these do not provide much information on the sentiment of tweets and will only add unnecessary complexity to our models.
              </p>
              <p class="project-details__desc-para">
                First, we create a string version containing a list of words for further EDA,
              </p>
              <div class="div-code">
                <p>df_train['temp_list'] = df_train['text'].apply(lambda x:str(x).split())</p>
                <p>&nbsp;</p>
                <p># Str version for EDA</p>
                <p>def correct_spellings(text):</p>
                <p>def remove_stopword(x):</p>
                <p>&emsp;return [y for y in x if y not in stopwords.words('english')]</p>
                <p>&emsp;df_train['temp_list'] = df_train['temp_list'].apply(lambda x:remove_stopword(x))</p>
              </div>
              <p class="project-details__desc-para">
                we also create a text version which will be used for TF-IDF vectorisation later.
              </p>
              <div class="div-code">
                <p># Text version for tfidf vectorisation</p>
                <p>import nltk</p>
                <p>nltk.download('stopwords')</p>
                <p>def remove_stopword(x):</p>
                <p>&emsp;new = ""</p>
                <p>&emsp;for y in x.split():</p>
                <p>&emsp;&emsp;if y not in stopwords.words('english'):</p>
                <p>&emsp;&emsp;&emsp;new = f'{new}{y} '</p>
                <p>&emsp;return new.rstrip()</p>
                <p>df_train['text'] = df_train['text'].apply(lambda x:remove_stopword(x))</p>
                <p>df_test['text'] = df_test['text'].apply(lambda x:remove_stopword(x))</p>
              </div>
              <h3 class="project-details__content-title_1">Post-Preprocessing Data Exploration</h3>
              <p class="project-details__desc-para">
                Now that preprocessing has been done we have a look at the most common words in each sentiment class. First, lets separate the sentiments.
              </p>
              <div class="div-code">
                <p>Pt_sent = df_train[df_train['sentiment']==1]</p>
                <p>Ng_sent = df_train[df_train['sentiment']==-1]</p>
                <p>Nt_sent = df_train[df_train['sentiment']==0]</p>
              </div>
              <p class="project-details__desc-para">
                Now we look at the most common words for each class.
              </p>
              <div class="div-code">
                <p>from collections import Counter</p>
                <p># Most common words for each sentiment</p>
                <p>for sentiment in [(Pt_sent, 'Greens'), (Nt_sent, 'Blues'), (Ng_sent, 'Reds')]:</p>
                <p>&emsp;top = Counter([item for sublist in sentiment[0]['temp_list'] for item in sublist])</p>
                <p>&emsp;temp_sent = pd.DataFrame(top.most_common(20))</p>
                <p>&emsp;temp_sent.columns = ['Common_words','count']</p>
                <p>&emsp;display(temp_sent.style.background_gradient(cmap=sentiment[1]))</p>
              </div>
              <img
                src="./assets/jpeg/tsa_most_common_words.jpg"
                alt="Project Image Hero"
                class="project-details__showcase-img"
              />
              <h3 class="project-details__content-title_1">Modeling</h3>
              <h3 class="project-details__content-title_2">TF-IDF</h3>
              <p class="project-details__desc-para">
                TF-IDF (Term Frequency-Inverse Document Frequency) is a common technique used in natural language processing (NLP) to represent the importance of words in a document. 
                It is typically used to transform text data into numerical vectors that can be used as input to machine learning models.
              </p>
              <p class="project-details__desc-para">
                We will be applying this vectorizer to our pre-processed data to extract features for our machine learning models.
              </p>
              <div class="div-code">
                <p>from sklearn.feature_extraction.text import TfidfVectorizer</p>
                <p>vectorizer = TfidfVectorizer(max_features=1024)</p>
                <p>X_train = vectorizer.fit_transform(df_train['text'])</p>
                <p>X_train = np.array(X_train.toarray())</p>
                <p>y_train = np.array(df_train['sentiment'])</p>
                <p>&nbsp;</p>
                <p>X_test = vectorizer.transform(df_test['text'])</p>
                <p>X_test = np.array(X_test.toarray())</p>
                <p>y_test = np.array(df_test['sentiment'])</p>
              </div>
              <h3 class="project-details__content-title_1">Logistic Regression</h3>
              <p class="project-details__desc-para">
                The first model we will be training is the logistic regression. 
                Logistic regression is a generalised linear model and assumes that the data follows a Bernoulli distribution. 
                Logistic regression solves for the parameters by maximising the likelihood function and applying gradient descent. 
                The structure of the logistic regression model is simple and interpretable, and the influence of different features on the final results can be seen from the weights of the features.
              </p>
              <p class="project-details__desc-para">
                We use the sklearn implementation of logistic regression and grid-search for hyperparameter tuning.
              </p>
              <div class="div-code">
                <p>from sklearn.model_selection import GridSearchCV</p>
                <p>from sklearn.linear_model import LogisticRegression</p>
                <p>grid={"C":np.linspace(0,10,11), "penalty":["l1","l2"]}# l1 lasso l2 ridge</p>
                <p>logreg=LogisticRegression()</p>
                <p>logreg_cv=GridSearchCV(logreg,grid,cv=10)</p>
                <p>logreg_cv.fit(X_train,y_train)</p>
                <p>&nbsp;</p>
                <p>print("tuned hpyerparameters :(best parameters) ",logreg_cv.best_params_)</p>
                <p>print("accuracy :",logreg_cv.best_score_)</p>
              </div>
              <p class="project-details__desc-para">
                We can then use this model to predict on the training set.
              </p>
              <div class="div-code">
                <p>clf = LogisticRegression(penalty=logreg_cv.best_params_['penalty'], C=logreg_cv.best_params_['C'])</p>
                <p>clf.fit(X_train, y_train)</p>
                <p>y_pred = clf.predict(X_test)</p>
                <p>print('test acc: %.2f%%, test f1 score: %.4f' % (100 * accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='weighted')))</p>
              </div>
              <div class="div-list">
                <ul>
                  <li>
                    <p class="project-details__desc-list_ele">&bull; Test accuracy = 68.48%</p>
                  </li>
                  <li>
                    <p class="project-details__desc-list_ele">&bull; Test f1 score = 0.6849</p>
                  </li>
                </ul>
              </div>
              <p class="project-details__desc-para">
                Multilayer Perceptron - details to be added
              </p>
            </div> 
            <div class="project-details__tools-used">
              <h3 class="project-details__content-title_1">Tools Used</h3>
              <div class="skills">
                <div class="skills__skill">Python</div>
                <div class="skills__skill">Sklearn</div>
                <div class="skills__skill">Natural Language Processing</div>
                <div class="skills__skill">Machine Learning</div>
                <div class="skills__skill">Deep Learning</div>
                <div class="skills__skill">Jupyter</div>
                <div class="skills__skill">Matplotlib</div>
                <div class="skills__skill">Seaborn</div>
              </div>
            </div>
            <!---
            <div class="project-details__links">
              <h3 class="project-details__content-title_1">See Live</h3>
              <a
                href="#"
                class="btn btn--med btn--theme project-details__links-btn"
                target="_blank"
                >Live Link</a
              >
              <a
                href="#"
                class="btn btn--med btn--theme-inv project-details__links-btn"
                target="_blank"
                >Code Link</a
              >
            </div>
            --->
          </div>
        </div>
      </div>
    </section>
    <footer class="main-footer">
      <div class="main-container">
        <div class="main-footer__upper">
          <div class="main-footer__row main-footer__row-1">
            <h2 class="heading heading-sm main-footer__heading-sm">
              <span>Social</span>
            </h2>
            <div class="main-footer__social-cont">
              <a target="_blank" rel="noreferrer" href="#">
                <img
                  class="main-footer__icon"
                  src="./assets/png/linkedin-ico.png"
                  alt="icon"
                />
              </a>
              <a target="_blank" rel="noreferrer" href="#">
                <img
                  class="main-footer__icon"
                  src="./assets/png/github-ico.png"
                  alt="icon"
                />
              </a>
            </div>
          </div>
          <div class="main-footer__row main-footer__row-2">
            <h4 class="heading heading-sm text-lt">James Lu</h4>
            <p class="main-footer__short-desc">
              Getting the most out of data...
            </p>
          </div>
        </div>
      </div>
    </footer>
    <script src="./index.js"></script>
  </body>
</html>
